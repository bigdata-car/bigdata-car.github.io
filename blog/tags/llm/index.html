<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-tags-post-list-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.4.0">
<title data-rh="true">5 posts tagged with &quot;LLM&quot; | 자동차 산업을 위한 IT기술을 연구 합니다. </title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://bigdata-car.github.io/img/kadap.jpg"><meta data-rh="true" name="twitter:image" content="https://bigdata-car.github.io/img/kadap.jpg"><meta data-rh="true" property="og:url" content="https://bigdata-car.github.io/blog/tags/llm"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" property="og:title" content="5 posts tagged with &quot;LLM&quot; | 자동차 산업을 위한 IT기술을 연구 합니다. "><meta data-rh="true" name="description" content="Overview of Large Language Models"><meta data-rh="true" property="og:description" content="Overview of Large Language Models"><meta data-rh="true" name="docusaurus_tag" content="blog_tags_posts"><meta data-rh="true" name="docsearch:docusaurus_tag" content="blog_tags_posts"><link data-rh="true" rel="icon" href="/img/kadap.jpg"><link data-rh="true" rel="canonical" href="https://bigdata-car.github.io/blog/tags/llm"><link data-rh="true" rel="alternate" href="https://bigdata-car.github.io/blog/tags/llm" hreflang="en"><link data-rh="true" rel="alternate" href="https://bigdata-car.github.io/blog/tags/llm" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="자동차 산업을 위한 IT기술을 연구 합니다.  RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="자동차 산업을 위한 IT기술을 연구 합니다.  Atom Feed">




<link rel="alternate" type="application/rss+xml" href="/usecase/rss.xml" title="자동차 산업을 위한 IT기술을 연구 합니다.  RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/usecase/atom.xml" title="자동차 산업을 위한 IT기술을 연구 합니다.  Atom Feed">
<link rel="alternate" type="application/rss+xml" href="/faq/rss.xml" title="자동차 산업을 위한 IT기술을 연구 합니다.  RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/faq/atom.xml" title="자동차 산업을 위한 IT기술을 연구 합니다.  Atom Feed">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.fcbc6eba.css">
<script src="/assets/js/runtime~main.61e8b72e.js" defer="defer"></script>
<script src="/assets/js/main.9b86c789.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo_light.png" alt="KADaP" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/logo_dark.png" alt="KADaP" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div></a><a class="navbar__item navbar__link" href="/manual/intro">Manual</a><a class="navbar__item navbar__link" href="/usecase">UseCase</a><a class="navbar__item navbar__link" href="/faq">FAQ</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/bigdata-car" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">Recent posts</div><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/welcome">Welcome</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/LLM 정리 5">LLM 정리 5</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/LLM 정리 4">LLM 정리 4</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/LLM 정리 3">LLM 정리 3</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/LLM 정리 1">LLM 정리 1</a></li></ul></nav></aside><main class="col col--7"><header class="margin-bottom--xl"><h1>5 posts tagged with &quot;LLM&quot;</h1><p>Overview of Large Language Models</p><a href="/blog/tags">View All Tags</a></header><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/blog/LLM 정리 5">LLM 정리 5</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2024-07-08T00:00:00.000Z">July 8, 2024</time> · <!-- -->11 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><div class="avatar__intro"><div class="avatar__name"><a href="https://github.com/bigdata-car" target="_blank" rel="noopener noreferrer"><span>강병수</span></a></div><small class="avatar__subtitle">데이터플랫폼연구센터 책임연구원</small></div></div></div></div></header><div class="markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="2-llm은-어떻게-동작하는가">2. LLM은 어떻게 동작하는가<a class="hash-link" aria-label="Direct link to 2. LLM은 어떻게 동작하는가" title="Direct link to 2. LLM은 어떻게 동작하는가" href="/blog/tags/llm#2-llm은-어떻게-동작하는가">​</a></h2>
<ul>
<li>
<p>개요에서 언급한 것 처럼 LLM은 이전 토큰 세트를 기반으로 다음 토큰을 예측하도록 훈련되었습니다. 이는 생성기능을 활성화 하는 자동회귀 방식(autoregressive, 현재 생성된 토큰은 다음 토큰을 생성하기 위한 입력으로 거대언어모델에 재입력 됨)을 수행하여 생성을 가능하게 합니다.</p>
</li>
<li>
<p>첫 번째 단계에서 받은 프롬프트를 토큰화 하고 이를 임베딩으로 변환하는 작업이 수행 됩니다. 임베딩은 입력 텍스트의 벡터 표현입니다. 이러한 임베딩은 무작위로 초기화되어 입력 토큰의 비 의미론적인 벡터 형태를 나타냅니다. 그리고, 모델 훈련 과정에서 맥락화되는 학습이 수행됩니다.</p>
</li>
<li>
<p>다음으로, 레이어별 어텐션(attentation) 및 피드포워드 연산을 수행하여 최종적으로 어휘의 각 단어에 숫자 또는 로짓(logit)을 출력하거나(GPT-N, LLaMA 등의 디코더 모델) 의미론적 임베딩을 출력합니다(BERT와 같은 인코더 모델 및 RoBERTa, ELECTRA 등과 같은 변형 모델).</p>
</li>
<li>
<p>마지막으로 디코더 모델의 경우 다음 단계는 각 (정규화되지 않은) 로짓을 (정규화된) 확률 분포(Softmax 함수를 통해)로 변환하여   텍스트에서 다음에 올 단어를 결정하는 것입니다.</p>
</li>
<li>
<p>아래와 같이 단계를 더 자세히 살펴보겠습니다 :</p>
<ol>
<li>
<p><strong>토큰화</strong> :</p>
<ul>
<li>LLM이 처리를 하기 전에 원시 입력 텍스트는 더 작은 단위(종종 하위 단어 또는 단어)로 토큰화 하여 모델이 인식할 수 있는 조각으로 입력을 나눕니다.</li>
<li>모델에는 고정된 어휘목록(vocabulary)이 있습니다. 따라서, 토큰화 단계는 입력이 어휘목록과 일치하는 형식이 되도록 보장하기 때문에 매우 중요합니다.</li>
<li>GPT-3.5 및 GPT-4용 OpenAI 토크나이저는 <a href="https://platform.openai.com/tokenizer" target="_blank" rel="noopener noreferrer">여기</a>에서 찾을 수 있습니다.</li>
<li>자세한 내용은 <a href="https://aman.ai/primers/ai/tokenizer/" target="_blank" rel="noopener noreferrer">토큰화에 대한 입문서</a>를 참조하세요.</li>
</ul>
</li>
<li>
<p><strong>임베딩</strong> :</p>
<ul>
<li>각 토큰은 임베딩 매트릭스를 사용하여 고차원 벡터에 매핑됩니다. 이 벡터 표현은 토큰의 맥락적 의미를 포착하며 모델의 다음 레이어에 입력으로 사용됩니다.</li>
<li>토큰의 순서에 대한 정보를 모델에 제공하기 위해 매핑된 임베딩에 위치 인코딩(positional encoding)이 추가됩니다. 이는 트랜스포머와 같은 모델이 고유한 순서 인식을 갖고 있지 않기 때문에 특히 중요합니다.</li>
</ul>
</li>
<li>
<p><strong>트랜스포머 구조</strong> :</p>
<ul>
<li>대부분의 최신 LLM의 핵심은 트랜스포머 구조입니다.</li>
<li>트랜스포머는 여러 레이어로 구성되어 있으며, 각 레이어에는 두 가지 주요 구성 요소가 있습니다 : multi-head self-attention 메커니즘과 position-wise feed-forward network 입니다.</li>
<li>자기 어텐션 메커니즘(self-attention mechanism)은 각 토큰들이 자신과 관련해 중요성을 갖는 다른 토큰들에게 가중치를 부여할 수 있게 합니다. 이는 본질적으로 주어진 토큰과 관련있는 특정 부분에 대해 모델이 &quot;주의를 기울일&quot; 수 있도록 합니다.</li>
<li>어텐션 연산된 결과는, 각 위치에서 독립적으로 피드포워드 신경망으로 전달됩니다.</li>
<li>자세한 내용은 <a href="https://aman.ai/primers/ai/transformers/" target="_blank" rel="noopener noreferrer">트랜스포머 아키텍처에 대한 입문서</a>를 참조하세요.</li>
</ul>
</li>
<li>
<p><strong>잔차연결 (Residual Connection)</strong> :</p>
<ul>
<li>모델의 각 하위 계층(예: 자기 어텐션 또는 피드포워드 신경망)은 주변에 잔여 연결이 적용된 후 계층 정규화가 수행됩니다. 이는 활성화를 안정화하고 훈련 속도를 높이는 데 도움이 됩니다.</li>
</ul>
</li>
<li>
<p><strong>출력 레이어</strong> :</p>
<ul>
<li>모든 트랜스포머 레이어를 통과한 후, 각 토큰의 최종 표현은 모델의 어휘목록에 있는 각 단어에 대응하는 로짓 벡터로 변환됩니다.</li>
<li>이러한 로짓은 어휘 목록의 각 단어들이 시퀀스의 다음 단어가 될 가능성을 설명합니다.</li>
</ul>
</li>
<li>
<p><strong>확률분포</strong> :</p>
<ul>
<li>로짓을 확률로 변환하기 위해 Softmax 함수가 적용됩니다. 이는 모두 0과 1 사이에 있고 합이 1이 되도록 로짓을 정규화합니다.</li>
<li>어휘 목록의 단어들 중 확률이 가장 높은 단어가 시퀀스의 다음 단어로 선택될 수 있습니다.</li>
</ul>
</li>
<li>
<p><strong>디코딩 (Decoding)</strong> :</p>
<ul>
<li>적용되는 상황에 따라 일관되고 문맥에 맞는 시퀀스를 생성하기 위하여, 그리디 디코딩(greedy decoding), 빔 검색(beam search), Top-K 샘플링(top-k sampling)과 같은 다양한 디코딩 전략이 사용됩니다.</li>
<li>자세한 내용은 <a href="https://aman.ai/primers/ai/token-sampling/" target="_blank" rel="noopener noreferrer">토큰 샘플링 방법</a>에 대한 입문서를 참조하세요.</li>
</ul>
</li>
</ol>
</li>
<li>
<p>여러 단계의 프로세스를 통해, LLM은 인간과 유사한 텍스트를 생성하고, 맥락을 이해하고, 프롬프트에 대한 관련 응답이나 완성을 제공할 수 있습니다.</p>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="21-llm-학습-단계">2.1. LLM 학습 단계<a class="hash-link" aria-label="Direct link to 2.1. LLM 학습 단계" title="Direct link to 2.1. LLM 학습 단계" href="/blog/tags/llm#21-llm-학습-단계">​</a></h3>
<ul>
<li>상위 수준에서, LLMs의 훈련에 포함되는 단계는 다음과 같습니다:<!-- -->
<ol>
<li><strong>문서(코퍼스, corpus) 준비</strong> : 뉴스 기사, 소셜 미디어 게시물, 웹 문서 등 대규모 텍스트 데이터 모음을 수집합니다.</li>
<li><strong>토큰화</strong> : 텍스트를 토큰이라고 하는 개별 단어 또는 하위 단어로 분할합니다.</li>
<li><strong>임베딩 생성</strong> : 일반적으로 훈련을 처음 시작할 때 PyTorch의 <a href="https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html" target="_blank" rel="noopener noreferrer"><code>nn.Embedding</code></a> 클래스를 통해 랜덤하게 초기화된 임베딩 테이블을 사용합니다. 또한, Word2Vec, GloVe, FastText 등과 같은 사전 훈련된 임베딩도 사용할 수 있습니다. 이러한 임베딩은 입력 토큰의 맥락화되지 않은 벡터 형식을 나타냅니다.</li>
<li><strong>신경망 훈련</strong> : 입력 토큰에 대한 신경망 모델을 훈련합니다.<!-- -->
<ul>
<li>BERT 및 그 변형과 같은 인코더 모델의 경우 모델은 마스킹된 특정 단어의 전후 맥락(주변 단어)을 예측하는 방법을 학습합니다.</li>
<li>BERT는 특히 마스킹된 단어를 예측하는 마스크드 언어 모델링 작업(Masked Language Modeling task 또는 Cloze task)과 다음 문장 예측 작업으로 훈련되 었습니다; <a href="https://aman.ai/primers/ai/bert/" target="_blank" rel="noopener noreferrer">BERT 입문서</a>에 설명되어 있습니다.</li>
<li>GPT-N, LLaMA 등과 같은 디코더 모델의 경우 주어진 이전 토큰들의 맥락을 고려하여 시퀀스의 다음 토큰을 예측하는 방법을 학습합니다.</li>
</ul>
</li>
</ol>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="22-추론-reasoning">2.2. 추론 (Reasoning)<a class="hash-link" aria-label="Direct link to 2.2. 추론 (Reasoning)" title="Direct link to 2.2. 추론 (Reasoning)" href="/blog/tags/llm#22-추론-reasoning">​</a></h3>
<ul>
<li>LLM에서 추론이 어떻게 작동하는지 살펴보겠습니다; 우리는 추론을 “증거와 논리를 사용하여 추론하는 능력”으로 정의할 것입니다. <a href="https://arxiv.org/pdf/2302.07842" target="_blank" rel="noopener noreferrer">(source)</a></li>
<li>추론에는 상식적 추론이나 수학적 추론과 같이 다양한 종류가 있습니다.</li>
<li>마찬가지로, 모델에서 추론을 이끌어내는 방법 또한 다양하며 그 중 하나는 여기에서 언급하는 생각의 사슬(chain-of-thought) 프롬프팅 입니다.</li>
<li>추론과 사실적 정보를 분리하여 최종 결과에 대한 기여도를 분석하는 것은 간단한 일이 아니기 때문에, LLM이 최종 예측을 위해 얼마나 많은 추론을 하는지 아직 알 수 없다는 점을 유의하는 것이 중요합니다.</li>
</ul></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a title="Overview of Large Language Models" class="tag_zVej tagRegular_sFm0" href="/blog/tags/llm">LLM</a></li></ul></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/blog/LLM 정리 4">LLM 정리 4</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2024-07-07T00:00:00.000Z">July 7, 2024</time> · <!-- -->11 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><div class="avatar__intro"><div class="avatar__name"><a href="https://github.com/bigdata-car" target="_blank" rel="noopener noreferrer"><span>강병수</span></a></div><small class="avatar__subtitle">데이터플랫폼연구센터 책임연구원</small></div></div></div></div></header><div class="markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="2-llm은-어떻게-동작하는가">2. LLM은 어떻게 동작하는가<a class="hash-link" aria-label="Direct link to 2. LLM은 어떻게 동작하는가" title="Direct link to 2. LLM은 어떻게 동작하는가" href="/blog/tags/llm#2-llm은-어떻게-동작하는가">​</a></h2>
<ul>
<li>
<p>개요에서 언급한 것 처럼 LLM은 이전 토큰 세트를 기반으로 다음 토큰을 예측하도록 훈련되었습니다. 이는 생성기능을 활성화 하는 자동회귀 방식(autoregressive, 현재 생성된 토큰은 다음 토큰을 생성하기 위한 입력으로 거대언어모델에 재입력 됨)을 수행하여 생성을 가능하게 합니다.</p>
</li>
<li>
<p>첫 번째 단계에서 받은 프롬프트를 토큰화 하고 이를 임베딩으로 변환하는 작업이 수행 됩니다. 임베딩은 입력 텍스트의 벡터 표현입니다. 이러한 임베딩은 무작위로 초기화되어 입력 토큰의 비 의미론적인 벡터 형태를 나타냅니다. 그리고, 모델 훈련 과정에서 맥락화되는 학습이 수행됩니다.</p>
</li>
<li>
<p>다음으로, 레이어별 어텐션(attentation) 및 피드포워드 연산을 수행하여 최종적으로 어휘의 각 단어에 숫자 또는 로짓(logit)을 출력하거나(GPT-N, LLaMA 등의 디코더 모델) 의미론적 임베딩을 출력합니다(BERT와 같은 인코더 모델 및 RoBERTa, ELECTRA 등과 같은 변형 모델).</p>
</li>
<li>
<p>마지막으로 디코더 모델의 경우 다음 단계는 각 (정규화되지 않은) 로짓을 (정규화된) 확률 분포(Softmax 함수를 통해)로 변환하여 텍스트에서 다음에 올 단어를 결정하는 것입니다.</p>
</li>
<li>
<p>아래와 같이 단계를 더 자세히 살펴보겠습니다 :</p>
<ol>
<li>
<p><strong>토큰화</strong> :</p>
<ul>
<li>LLM이 처리를 하기 전에 원시 입력 텍스트는 더 작은 단위(종종 하위 단어 또는 단어)로 토큰화 하여 모델이 인식할 수 있는 조각으로 입력을 나눕니다.</li>
<li>모델에는 고정된 어휘목록(vocabulary)이 있습니다. 따라서, 토큰화 단계는 입력이 어휘목록과 일치하는 형식이 되도록 보장하기 때문에 매우 중요합니다.</li>
<li>GPT-3.5 및 GPT-4용 OpenAI 토크나이저는 <a href="https://platform.openai.com/tokenizer" target="_blank" rel="noopener noreferrer">여기</a>에서 찾을 수 있습니다.</li>
<li>자세한 내용은 <a href="https://aman.ai/primers/ai/tokenizer/" target="_blank" rel="noopener noreferrer">토큰화에 대한 입문서</a>를 참조하세요.</li>
</ul>
</li>
<li>
<p><strong>임베딩</strong> :</p>
<ul>
<li>각 토큰은 임베딩 매트릭스를 사용하여 고차원 벡터에 매핑됩니다. 이 벡터 표현은 토큰의 맥락적 의미를 포착하며 모델의 다음 레이어에 입력으로 사용됩니다.</li>
<li>토큰의 순서에 대한 정보를 모델에 제공하기 위해 매핑된 임베딩에 위치 인코딩(positional encoding)이 추가됩니다. 이는 트랜스포머와 같은 모델이 고유한 순서 인식을 갖고 있지 않기 때문에 특히 중요합니다.</li>
</ul>
</li>
<li>
<p><strong>트랜스포머 구조</strong> :</p>
<ul>
<li>대부분의 최신 LLM의 핵심은 트랜스포머 구조입니다.</li>
<li>트랜스포머는 여러 레이어로 구성되어 있으며, 각 레이어에는 두 가지 주요 구성 요소가 있습니다 : multi-head self-attention 메커니즘과 position-wise feed-forward network 입니다.</li>
<li>자기 어텐션 메커니즘(self-attention mechanism)은 각 토큰들이 자신과 관련해 중요성을 갖는 다른 토큰들에게 가중치를 부여할 수 있게 합니다. 이는 본질적으로 주어진 토큰과 관련있는 특정 부분에 대해 모델이 &quot;주의를 기울일&quot; 수 있도록 합니다.</li>
<li>어텐션 연산된 결과는, 각 위치에서 독립적으로 피드포워드 신경망으로 전달됩니다.</li>
<li>자세한 내용은 <a href="https://aman.ai/primers/ai/transformers/" target="_blank" rel="noopener noreferrer">트랜스포머 아키텍처에 대한 입문서</a>를 참조하세요.</li>
</ul>
</li>
<li>
<p><strong>잔차연결 (Residual Connection)</strong> :</p>
<ul>
<li>모델의 각 하위 계층(예: 자기 어텐션 또는 피드포워드 신경망)은 주변에 잔여 연결이 적용된 후 계층 정규화가 수행됩니다. 이는 활성화를 안정화하고 훈련 속도를 높이는 데 도움이 됩니다.</li>
</ul>
</li>
<li>
<p><strong>출력 레이어</strong> :</p>
<ul>
<li>모든 트랜스포머 레이어를 통과한 후, 각 토큰의 최종 표현은 모델의 어휘목록에 있는 각 단어에 대응하는 로짓 벡터로 변환됩니다.</li>
<li>이러한 로짓은 어휘 목록의 각 단어들이 시퀀스의 다음 단어가 될 가능성을 설명합니다.</li>
</ul>
</li>
<li>
<p><strong>확률분포</strong> :</p>
<ul>
<li>로짓을 확률로 변환하기 위해 Softmax 함수가 적용됩니다. 이는 모두 0과 1 사이에 있고 합이 1이 되도록 로짓을 정규화합니다.</li>
<li>어휘 목록의 단어들 중 확률이 가장 높은 단어가 시퀀스의 다음 단어로 선택될 수 있습니다.</li>
</ul>
</li>
<li>
<p><strong>디코딩 (Decoding)</strong> :</p>
<ul>
<li>적용되는 상황에 따라 일관되고 문맥에 맞는 시퀀스를 생성하기 위하여, 그리디 디코딩(greedy decoding), 빔 검색(beam search), Top-K 샘플링(top-k sampling)과 같은 다양한 디코딩 전략이 사용됩니다.</li>
<li>자세한 내  용은 <a href="https://aman.ai/primers/ai/token-sampling/" target="_blank" rel="noopener noreferrer">토큰 샘플링 방법</a>에 대한 입문서를 참조하세요.</li>
</ul>
</li>
</ol>
</li>
<li>
<p>여러 단계의 프로세스를 통해, LLM은 인간과 유사한 텍스트를 생성하고, 맥락을 이해하고, 프롬프트에 대한 관련 응답이나 완성을 제공할 수 있습니다.</p>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="21-llm-학습-단계">2.1. LLM 학습 단계<a class="hash-link" aria-label="Direct link to 2.1. LLM 학습 단계" title="Direct link to 2.1. LLM 학습 단계" href="/blog/tags/llm#21-llm-학습-단계">​</a></h3>
<ul>
<li>상위 수준에서, LLMs의 훈련에 포함되는 단계는 다음과 같습니다:<!-- -->
<ol>
<li><strong>문서(코퍼스, corpus) 준비</strong> : 뉴스 기사, 소셜 미디어 게시물, 웹 문서 등 대규모 텍스트 데이터 모음을 수집합니다.</li>
<li><strong>토큰화</strong> : 텍스트를 토큰이라고 하는 개별 단어 또는 하위 단어로 분할합니다.</li>
<li><strong>임베딩 생성</strong> : 일반적으로 훈련을 처음 시작할 때 PyTorch의 <a href="https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html" target="_blank" rel="noopener noreferrer"><code>nn.Embedding</code></a> 클래스를 통해 랜덤하게 초기화된 임베딩 테이블을 사용합니다. 또한, Word2Vec, GloVe, FastText 등과 같은 사전 훈련된 임베딩도 사용할 수 있습니다. 이러한 임베딩은 입력 토큰의 맥락화되지 않은 벡터 형식을 나타냅니다.</li>
<li><strong>신경망 훈련</strong> : 입력 토큰에 대한 신경망 모델을 훈련합니다.<!-- -->
<ul>
<li>BERT 및 그 변형과 같은 인코더 모델의 경우 모델은 마스킹된 특정 단어의 전후 맥락(주변 단어)을 예측하는 방법을 학습합니다.</li>
<li>BERT는 특히 마스킹된 단어를 예측하는 마스크드 언어 모델링 작업(Masked Language Modeling task 또는 Cloze task)과 다음 문장 예측 작업으로 훈련되었습니다; <a href="https://aman.ai/primers/ai/bert/" target="_blank" rel="noopener noreferrer">BERT 입문서</a>에 설명되어 있습니다.</li>
<li>GPT-N, LLaMA 등과 같은 디코더 모델의 경우 주어진 이전 토큰들의 맥락을 고려하여 시퀀스의 다음 토큰을 예측하는 방법을 학습합니다.</li>
</ul>
</li>
</ol>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="22-추론-reasoning">2.2. 추론 (Reasoning)<a class="hash-link" aria-label="Direct link to 2.2. 추론 (Reasoning)" title="Direct link to 2.2. 추론 (Reasoning)" href="/blog/tags/llm#22-추론-reasoning">​</a></h3>
<ul>
<li>LLM에서 추론이 어떻게 작동하는지 살펴보겠습니다; 우리는 추론을 “증거와 논리를 사용하여 추론하는 능력”으로 정의할 것입니다. <a href="https://arxiv.org/pdf/2302.07842" target="_blank" rel="noopener noreferrer">(source)</a></li>
<li>추론에는 상식적 추론이나 수학적 추론과 같이 다양한 종류가 있습니다.</li>
<li>마찬가지로, 모델에서 추론을 이끌어내는 방법 또한 다양하며 그 중 하나는 여기에서 언급하는 생각의 사슬(chain-of-thought) 프롬프팅 입니다.</li>
<li>추론과 사실적 정보를 분리하여 최종 결과에 대한 기여도를 분석하는 것은 간단한 일이 아니기 때문에, LLM이 최종 예측을 위해 얼마나 많은 추론을 하는지 아직 알 수 없다는 점을 유의하는 것이 중요합니다.</li>
</ul></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a title="Overview of Large Language Models" class="tag_zVej tagRegular_sFm0" href="/blog/tags/llm">LLM</a></li></ul></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/blog/LLM 정리 3">LLM 정리 3</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2024-07-06T00:00:00.000Z">July 6, 2024</time> · <!-- -->11 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><div class="avatar__intro"><div class="avatar__name"><a href="https://github.com/bigdata-car" target="_blank" rel="noopener noreferrer"><span>강병수</span></a></div><small class="avatar__subtitle">데이터플랫폼연구센터 책임연구원</small></div></div></div></div></header><div class="markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="2-llm은-어떻게-동작하는가">2. LLM은 어떻게 동작하는가<a class="hash-link" aria-label="Direct link to 2. LLM은 어떻게 동작하는가" title="Direct link to 2. LLM은 어떻게 동작하는가" href="/blog/tags/llm#2-llm은-어떻게-동작하는가">​</a></h2>
<ul>
<li>
<p>개요에서 언급한 것 처럼 LLM은 이전 토큰 세트를 기반으로 다음 토큰을 예측하도록 훈련되었습니다. 이는 생성기능을 활성화 하는 자동회귀 방식(autoregressive, 현재 생성된 토큰은 다음 토큰을 생성하기 위한 입력으로 거대언어모델에 재입력 됨)을 수행하여 생성을 가능하게 합니다.</p>
</li>
<li>
<p>첫 번째 단계에서 받은 프롬프트를 토큰화 하고 이를 임베딩으로 변환하는 작업이 수행 됩니다. 임베딩은 입력 텍스트의 벡터 표현입니다. 이러한 임베딩은 무작위로 초기화되어 입력 토큰의 비 의미론적인 벡터 형태를 나타냅니다. 그리고, 모델 훈련 과정에서 맥락화되는 학습이 수행됩니다.</p>
</li>
<li>
<p>다음으로, 레이어별 어텐션(attentation) 및 피드포워드 연산을 수행하여 최종적으로 어휘의 각 단어에 숫자 또는 로짓(logit)을 출력하거나(GPT-N, LLaMA 등의 디코더 모델) 의미론적 임베딩을 출력합니다(BERT와 같은 인코더 모델 및 RoBERTa, ELECTRA 등과 같은 변형 모델).</p>
</li>
<li>
<p>마지막으로 디코더 모델의 경우 다음 단계는 각 (정규화되지 않은) 로짓을 (정규화된) 확률 분포(Softmax 함수를 통해)로 변환하여 텍스트에서 다음에 올 단어를 결정하는 것입니다.</p>
</li>
<li>
<p>아래와 같이 단계를 더 자세히 살펴보겠습니다 :</p>
<ol>
<li>
<p><strong>토큰화</strong> :</p>
<ul>
<li>LLM이 처리를 하기 전에 원시 입력 텍스트는 더 작은 단위(종종 하위 단어 또는 단어)로 토큰화 하여 모델이 인식할 수 있는 조각으로 입력을 나눕니다.</li>
<li>모델에는 고정된 어휘목록(vocabulary)이 있습니다. 따라서, 토큰화 단계는 입력이 어휘목록과 일치하는 형식이 되도록 보장하기 때문에 매우 중요합니다.</li>
<li>GPT-3.5 및 GPT-4용 OpenAI 토크나이저는 <a href="https://platform.openai.com/tokenizer" target="_blank" rel="noopener noreferrer">여기</a>에서 찾을 수 있습니다.</li>
<li>자세한 내용은 <a href="https://aman.ai/primers/ai/tokenizer/" target="_blank" rel="noopener noreferrer">토큰화에 대한 입문서</a>를 참조하세요.</li>
</ul>
</li>
<li>
<p><strong>임베딩</strong> :</p>
<ul>
<li>각 토큰은 임베딩 매트릭스를 사용하여 고차원 벡터에 매핑됩니다. 이 벡터 표현은 토큰의 맥락적 의미를 포착하며 모델의 다음 레이어에 입력으로 사용됩니다.</li>
<li>토큰의 순서에 대한 정보를 모델에 제공하기 위해 매핑된 임베딩에 위치 인코딩(positional encoding)이 추가됩니다. 이는 트랜스포머와 같은 모델이 고유한 순서 인식을 갖고 있지 않기 때문에 특히 중요합니다.</li>
</ul>
</li>
<li>
<p><strong>트랜스포머 구조</strong> :</p>
<ul>
<li>대부분의 최신 LLM의 핵심은 트랜스포머 구조입니다.</li>
<li>트랜스포머는 여러 레이어로 구성되어 있으며, 각 레이어에는 두 가지 주요 구성 요소가 있습니다 : multi-head self-attention 메커니즘과 position-wise feed-forward network 입니다.</li>
<li>자기 어텐션 메커니즘(self-attention mechanism)은 각 토큰들이 자신과 관련해 중요성을 갖는 다른 토큰들에게 가중치를 부여할 수 있게 합니다. 이는 본질적으로 주어진 토큰과 관련있는 특정 부분에 대해 모델이 &quot;주의를 기울일&quot; 수 있도록 합니다.</li>
<li>어텐션 연산된 결과는, 각 위치에서 독립적으로 피드포워드 신경망으로 전달됩니다.</li>
<li>자세한 내용은 <a href="https://aman.ai/primers/ai/transformers/" target="_blank" rel="noopener noreferrer">트랜스포머 아키텍처에 대한 입문서</a>를 참조하세요.</li>
</ul>
</li>
<li>
<p><strong>잔차연결 (Residual Connection)</strong> :</p>
<ul>
<li>모델의 각 하위 계층(예: 자기 어텐션 또는 피드포워드 신경망)은 주변에 잔여 연결이 적용된 후 계층 정규화가 수행됩니다. 이는 활성화를 안정화하고 훈련 속도를 높이는 데 도움이 됩니다.</li>
</ul>
</li>
<li>
<p><strong>출력 레이어</strong> :</p>
<ul>
<li>모든 트랜스포머 레이어를 통과한 후, 각 토큰의 최종 표현은 모델의 어휘목록에 있는 각 단어에 대응하는 로짓 벡터로 변환됩니다.</li>
<li>이러한 로짓은 어휘 목록의 각 단어들이 시퀀스의 다음 단어가 될 가능성을 설명합니다.</li>
</ul>
</li>
<li>
<p><strong>확률분포</strong> :</p>
<ul>
<li>로짓을 확률로 변환하기 위해 Softmax 함수가 적용됩니다. 이는 모두 0과 1 사이에 있고 합이 1이 되도록 로짓을 정규화합니다.</li>
<li>어휘 목록의 단어들 중 확률이 가장 높은 단어가 시퀀스의 다음 단어로 선택될 수 있습니다.</li>
</ul>
</li>
<li>
<p><strong>디코딩 (Decoding)</strong> :</p>
<ul>
<li>적용되는 상황에 따라 일관되고 문맥에 맞는 시퀀스를 생성하기 위하여, 그리디 디코딩(greedy decoding), 빔 검색(beam search), Top-K 샘플링(top-k sampling)과   같은 다양한 디코딩 전략이 사용됩니다.</li>
<li>자세한 내용은 <a href="https://aman.ai/primers/ai/token-sampling/" target="_blank" rel="noopener noreferrer">토큰 샘플링 방법</a>에 대한 입문서를 참조하세요.</li>
</ul>
</li>
</ol>
</li>
<li>
<p>여러 단계의 프로세스를 통해, LLM은 인간과 유사한 텍스트를 생성하고, 맥락을 이해하고, 프롬프트에 대한 관련 응답이나 완성을 제공할 수 있습니다.</p>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="21-llm-학습-단계">2.1. LLM 학습 단계<a class="hash-link" aria-label="Direct link to 2.1. LLM 학습 단계" title="Direct link to 2.1. LLM 학습 단계" href="/blog/tags/llm#21-llm-학습-단계">​</a></h3>
<ul>
<li>상위 수준에서, LLMs의 훈련에 포함되는 단계는 다음과 같습니다:<!-- -->
<ol>
<li><strong>문서(코퍼스, corpus) 준비</strong> : 뉴스 기사, 소셜 미디어 게시물, 웹 문서 등 대규모 텍스트 데이터 모음을 수집합니다.</li>
<li><strong>토큰화</strong> : 텍스트를 토큰이라고 하는 개별 단어 또는 하위 단어로 분할합니다.</li>
<li><strong>임베딩 생성</strong> : 일반적으로 훈련을 처음 시작할 때 PyTorch의 <a href="https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html" target="_blank" rel="noopener noreferrer"><code>nn.Embedding</code></a> 클래스를 통해 랜덤하게 초기화된 임베딩 테이블을 사용합니다. 또한, Word2Vec, GloVe, FastText 등과 같은 사전 훈련된 임베딩도 사용할 수 있습니다. 이러한 임베딩은 입력 토큰의 맥락화되지 않은 벡터 형식을 나타냅니다.</li>
<li><strong>신경망 훈련</strong> : 입력 토큰에 대한 신경망 모델을 훈련합니다.<!-- -->
<ul>
<li>BERT 및 그 변형과 같은 인코더 모델의 경우 모델은 마스킹된 특정 단어의 전후 맥락(주변 단어)을 예측하는 방법을 학습합니다.</li>
<li>BERT는 특히 마스킹된 단어를 예측하는 마스크드 언어 모델링 작업(Masked Language Modeling task 또는 Cloze task)과 다음 문장 예측 작업으로 훈련되었습니다; <a href="https://aman.ai/primers/ai/bert/" target="_blank" rel="noopener noreferrer">BERT 입문서</a>에 설명되어 있습니다.</li>
<li>GPT-N, LLaMA 등과 같은 디코더 모델의 경우 주어진 이전 토큰들의 맥락을 고려하여 시퀀스의 다음 토큰을 예측하는 방법을 학습합니다.</li>
</ul>
</li>
</ol>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="22-추론-reasoning">2.2. 추론 (Reasoning)<a class="hash-link" aria-label="Direct link to 2.2. 추론 (Reasoning)" title="Direct link to 2.2. 추론 (Reasoning)" href="/blog/tags/llm#22-추론-reasoning">​</a></h3>
<ul>
<li>LLM에서 추론이 어떻게 작동하는지 살펴보겠습니다; 우리는 추론을 “증거와 논리를 사용하여 추론하는 능력”으로 정의할 것입니다. <a href="https://arxiv.org/pdf/2302.07842" target="_blank" rel="noopener noreferrer">(source)</a></li>
<li>추론에는 상식적 추론이나 수학적 추론과 같이 다양한 종류가 있습니다.</li>
<li>마찬가지로, 모델에서 추론을 이끌어내는 방법 또한 다양하며 그 중 하나는 여기에서 언급하는 생각의 사슬(chain-of-thought) 프롬프팅 입니다.</li>
<li>추론과 사실적 정보를 분리하여 최종 결과에 대한 기여도를 분석하는 것은 간단한 일이 아니기 때문에, LLM이 최종 예측을 위해 얼마나 많은 추론을 하는지 아직 알 수 없다는 점을 유의하는 것이 중요합니다.</li>
</ul></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a title="Overview of Large Language Models" class="tag_zVej tagRegular_sFm0" href="/blog/tags/llm">LLM</a></li></ul></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/blog/LLM 정  리 1">LLM 정리 1</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2024-07-05T00:00:00.000Z">July 5, 2024</time> · <!-- -->21 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><div class="avatar__intro"><div class="avatar__name"><a href="https://github.com/bigdata-car" target="_blank" rel="noopener noreferrer"><span>강병수</span></a></div><small class="avatar__subtitle">데이터플랫폼연구센터 책임연구원</small></div></div></div></div></header><div class="markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="0-개요">0. 개요<a class="hash-link" aria-label="Direct link to 0. 개요" title="Direct link to 0. 개요" href="/blog/tags/llm#0-개요">​</a></h2>
<ul>
<li>
<p>LLM(Large Language Model)은 트랜스포머(transformer) 아키텍처를 활용하는 심층 신경망입니다. LLM은 엄청난 양의 비정형 데이터를 비지도 학습한 파운데이션 모델(foundation model)의 한 종류이며 파인튜닝(fine-tuning)을 통해 다향한 종류의 downstream task 모델로 변형될 수 있습니다.</p>
</li>
<li>
<p>트랜스포머 구조는 크게 인코더 모델(encoder 모델)과 디코더 모델(decoder model)로 구성됩니다. 두 모델을 구조적인 측면으로 바라보면 몇 가지 차이점을 제외하고는 거의 동일한 구조를 가지고 있습니다. (자세한 내용은 <a href="https://aman.ai/primers/ai/transformers/#transformer-encoder-and-decoder" target="_blank" rel="noopener noreferrer">Transformer</a> 입문과 <a href="https://aman.ai/primers/ai/autoregressive-vs-autoencoder-models/" target="_blank" rel="noopener noreferrer">Autoregressive vs. Autoencoder Models</a>를 참조하세요)</p>
</li>
<li>
<p>아울러, 생성형 인공지능은 디코더 기반 모델이 주로 사용되고 있기 때문에, 본 글에서는 인코더 모델(예: BERT 및 그 변형) 보다는 디코더 모델(예: GPT-x)에 더 중점을 두려고 합니다. 이후 LLM이라는 용어는 디코더 모델을 지칭하고자 합니다.</p>
</li>
<li>
<p>주어진 텍스트(prompt)가 주어졌을 때, LLM이 본질적으로 하는 일은 해당 시스템이 알고 있는 모든 단어목록(vocabulary - 단어의 부분 또는 토큰)에 대한 확률 분포를 계산하는 것입니다. 단어목록은 사람이 설계하여 모델에 부여하기 때문에 단어목록은 모델마다 다를 수 있으며 GPT-3의 경우 약 50,000개 토큰으로 구성된 단어목록이 있습니다. (<a href="https://aiguide.substack.com/p/on-detecting-whether-text-was-generated" target="_blank" rel="noopener noreferrer">Source</a>)</p>
</li>
<li>
<p>LLM은 여전히 환각현상(hallucination)이나 chain of thought(최근 개선이 있음)같은 수많은 제약 사항을 지니고 있지만, 해당 모델은 통계적 언어 모델링을 수행하도록 학습되었다는 점을 명심하는 것이 중요합니다.</p>
</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="1-임베딩embedding">1. 임베딩(Embedding)<a class="hash-link" aria-label="Direct link to 1. 임베딩(Embedding)" title="Direct link to 1. 임베딩(Embedding)" href="/blog/tags/llm#1-임베딩embedding">​</a></h2>
<ul>
<li>
<p>자연어 처리(NLP)에서의 임베딩은 단어 또는 문장의 의미론적 및 구문론적 속성을 포착하는 단어나 문장의 밀집된 벡터 표현입니다. 이러한 임베딩은 일반적으로 대규모 텍스트 모음을 BERT 및 그 변형, Word2Vec, Glove 또는 FastText와 같은 모델의 학습을 통해 얻을 수 있으며, 텍스트 정보를 기계 학습 알고리즘이 처리할 수 있는 형식으로 변환하는 방법을 제공합니다. 간단히 말해서, 임베딩은 단어의 의미론적 의미(내부적으로 하나 이상의 토큰으로 표시됨) 또는 문장의 의미론적 및 구문론적 속성을 조밀한 저차원 벡터로 표현하여 캡슐화합니다.</p>
</li>
<li>
<p>임베딩은 의미론적(contextualized)과 비 의미론적(non-contextualized)으로 구분됩니다. 의미론적의 경우 “bank”와 같은 다의어 단어는 주변의 문맥에 따라서 “finance” 또는 “river”의 의미를 갖는 임베딩으로 변환되어야 합니다. 따라서 의미론적 임베딩은 입력 토근 주변의 다른 토큰들의 함수로 나타내어 집니다. 반면에 비 의미론적에서 각 토큰들의 임베딩은 주변 문맥과 관계없이 사전학습을 통해 정적으로 얻어지며 downstream 작업에 활용될 수 있습니다.</p>
</li>
<li>
<p>토큰에 대한 임베딩을 얻으려면 각 단어에 대해 훈련된 모델에서 학습된 가중치를 추출합니다. 이러한 가중치는 단어 임베딩을 형성하며, 해당 임베딩은 각 단어의 조밀한 벡터로 표현됩니다.</p>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="11-의미론적-임베딩-vs-비-의미론적-임베딩">1.1. 의미론적 임베딩 vs. 비 의미론적 임베딩<a class="hash-link" aria-label="Direct link to 1.1. 의미론적 임베딩 vs. 비 의미론적 임베딩" title="Direct link to 1.1. 의미론적 임베딩 vs. 비 의미론적 임베딩" href="/blog/tags/llm#11-의미론적-임베딩-vs-비-의미론적-임베딩">​</a></h3>
<ul>
<li>BERT (Bidirectional Encoder Representations from Transformers)와 같은 트랜스포머 기반의 인코더 모델들은 의미론적 임베딩을 생성하도록 설계 되었습니다. 각 단어에 적정한 벡터를 할당하는 기존의 단어 임베딩(Word2Vec 또는 GloVe)과는 달리 이러한 모델들은 단어의 문맥(주변 단어들)을 고려합니다. 문맥 안에서 단어들이 어떻게 사용되는지에 따라 동일한 단어도 다른 뜻을 지니기 때문에 이러한 모델은 단어에 대한 더 풍부하고 미묘한 의미를 포착할 수 있습니다.</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="12-임베딩의-사용-예시">1.2. 임베딩의 사용 예시<a class="hash-link" aria-label="Direct link to 1.2. 임베딩의 사용 예시" title="Direct link to 1.2. 임베딩의 사용 예시" href="/blog/tags/llm#12-임베딩의-사용-예시">​</a></h3>
<ul>
<li>임베딩을 통해 특정작업 수행에 필요한 다양한 산술연산을 할수 있습니다 :<!-- -->
<ol>
<li><strong>단어 유사도(Word similarity)</strong> : 두 단어의 임베딩을 비교하여 유사성을 이해할 수 있습니다. 유사성 비교를 위해 코사인 유사도를 주로 사용합니다. 이는 두 벡터사이를 이루는 각도의 코사인 값을 측정하는 방법입니다. 두 벡터 사이에 코사인 값이 높다는 것은 두 단어들의 사용법이나 의미적인 측면에서 유사도가 높다는 것은 나타냅니다.</li>
<li><strong>단어 유추(Word analogy)</strong> : 벡터연산은 단어 유추작업에도 사용할 수 있습니다. 예를 들어 &quot;남자&quot;와 &quot;여자&quot;가 주어지고 이와 유사한 기준으로 왕은 무엇과 대응하는 지를 유추하는 문제가 주어졌을 때 <code>&quot;왕&quot; - &quot;남자&quot; + &quot;여자&quot;</code>의 산술연산을 각 단어에 대응되는 임베딩벡터의 연산을 통해서 <code>&quot;여왕&quot;</code> 이라는 답을 얻을 수 있습니다.</li>
<li><strong>문장 유사도(Sentence similarity)</strong> : 두 문장 간의 유사성을 측정하려면 문장의 총 의미를 캡처하도록 설계된 BERT와 같은 모델에서 생성된 특수 [CLS] 토큰 임베딩을 사용할 수 있습니다.<br>
<!-- -->또는 각 문장에 있는 모든 토큰의 임베딩을 평균화하는 평균 벡터를 만들어 해당 벡터들을 비교할 수 있습니다. 하지만 문장 유사성과 같은 문장 수준 작업의 경우 BERT 모델을 수정한 Sentence-BERT(SBERT)가 더 나은 선택인 경우가 많습니다.<br>
<code>SBERT</code>는 의미 공간에서 직접적으로 비교할 수 있는 문장 임베딩을 생성하도록 특별히 훈련되었으며, 이는 일반적으로 문장 수준 작업에서 더 나은 성능을 제공합니다. <code>SBERT</code>에서는 두 문장이 동시에 모델에 입력되므로 각 문장의 맥락을 다른 문장과 관련하여 이해할 수 있으므로 더 정확한 문장 임베딩이 가능합니다.</li>
</ol>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="13-임베딩을-통한-유사도-검색">1.3. 임베딩을 통한 유사도 검색<a class="hash-link" aria-label="Direct link to 1.3. 임베딩을 통한 유사도 검색" title="Direct link to 1.3. 임베딩을 통한 유사도 검색" href="/blog/tags/llm#13-임베딩을-통한-유사도-검색">​</a></h3>
<ul>
<li>
<p>인코더 모델의 출력으로 의미론적 임베딩을 얻게 됩니다. 두 단어간의 유사성 이해, 단어 유추등과 같은 다양한 작업을 위해 임베딩에 대한 산술연산을 할수 있습니다.</p>
</li>
<li>
<p>단어 유사도 작업에서는 단어들에 대한 각각의 의미론적 임베딩을 사용할 수 있습니다. 반면에 문장 유사도 작업에서는 [CLS] 토근의 아웃풋(output)을 사용할 수 있고 또한 모든 단어 토큰들의 임베딩을 평균화한 임베딩벡터를 사용할 수 있습니다. 하지만 문장 유사도 작업에서 최상의 성능을 얻으려면 Sentence BERT 또는 그 변형 모델들이 선호됩니다.</p>
</li>
<li>
<p>단어/문장 유사도는 두 단어/문장의 의미가 의미적으로 동일한 정도를 측정한 것입니다.</p>
</li>
<li>
<p>다음은 단어/문장 유사도에 대한 가장 일반적인 두 가지 척도입니다.<br>
<!-- -->(두 가지 모두 &quot;거리 척도&quot;는 아닙니다.)</p>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="131-내적-유사도dot-product-similarity">1.3.1. 내적 유사도(Dot Product Similarity)<a class="hash-link" aria-label="Direct link to 1.3.1. 내적 유사도(Dot Product Similarity)" title="Direct link to 1.3.1. 내적 유사도(Dot Product Similarity)" href="/blog/tags/llm#131-내적-유사도dot-product-similarity">​</a></h4>
<ul>
<li>
<p>두 벡터 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi></mrow><annotation encoding="application/x-tex">u</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">u</span></span></span></span>와 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">v</span></span></span></span>의 내적(dot product)은 다음과 같이 정의 됩니다.
</p><p align="center"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mo>⋅</mo><mi>v</mi><mo>=</mo><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>u</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>v</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>c</mi><mi>o</mi><mi>s</mi><mi>θ</mi></mrow><annotation encoding="application/x-tex">u \cdot v = ||u|| ||v|| cos\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4445em"></span><span class="mord mathnormal">u</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">∣∣</span><span class="mord mathnormal">u</span><span class="mord">∣∣∣∣</span><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="mord">∣∣</span><span class="mord mathnormal">cos</span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span></span></span></span></p><p></p>
</li>
<li>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>v</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">||v||=1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">∣∣</span><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="mord">∣∣</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1</span></span></span></span> 일때 내적을 활용한 유사도의 측정을 아래의 그림처럼 나타내면 이해가 쉬울 수 있습니다(참고, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi><mi>o</mi><mi>s</mi><mi>θ</mi><mo>=</mo><mfrac><mrow><mi>u</mi><mo>⋅</mo><mi>v</mi></mrow><mrow><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>u</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>v</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>u</mi><mo>⋅</mo><mi>v</mi></mrow><mrow><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>u</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">cos\theta = \frac{u \cdot v}{||u|| ||v||} = \frac{u \cdot v}{||u||}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">cos</span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.2251em;vertical-align:-0.52em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7051em"><span style="top:-2.655em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∣∣</span><span class="mord mathnormal mtight">u</span><span class="mord mtight">∣∣∣∣</span><span class="mord mathnormal mtight" style="margin-right:0.03588em">v</span><span class="mord mtight">∣∣</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">u</span><span class="mbin mtight">⋅</span><span class="mord mathnormal mtight" style="margin-right:0.03588em">v</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.52em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.2251em;vertical-align:-0.52em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7051em"><span style="top:-2.655em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∣∣</span><span class="mord mathnormal mtight">u</span><span class="mord mtight">∣∣</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">u</span><span class="mbin mtight">⋅</span><span class="mord mathnormal mtight" style="margin-right:0.03588em">v</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.52em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>). 그림에서 보면 내적은 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi></mrow><annotation encoding="application/x-tex">u</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">u</span></span></span></span>를 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">v</span></span></span></span> 방향으로 사영(projection)시킨 것이며 값의 크기는 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">v</span></span></span></span>와 이루는 각도, 그리고 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi></mrow><annotation encoding="application/x-tex">u</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">u</span></span></span></span>의 크기(노름 norm)에 의존합니다.</p>
</li>
</ul>
<p align="center"><img src="https://aman.ai/primers/ai/assets/LLM/dotp.png"></p>
<ul>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\theta = 0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">0</span></span></span></span>일 때, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi><mi>o</mi><mi>s</mi><mi>θ</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">cos\theta = 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">cos</span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">1</span></span></span></span>이 되고 두 벡터는 동일 선상에 있고 내적은 각 벡터의 크기간의 곱이 됩니다. 그리고 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span></span></span></span>가 직각일 때, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi><mi>o</mi><mi>s</mi><mi>θ</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">cos\theta =0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">cos</span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">0</span></span></span></span>이 되고 두 벡터는 직교하며 내적은 0이 됩니다. 일반적으로 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi><mi>o</mi><mi>s</mi><mi>θ</mi></mrow><annotation encoding="application/x-tex">cos\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">cos</span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span></span></span></span>는 두 벡터방향의 유사성을 나타냅니다 (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">-1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">−</span><span class="mord">1</span></span></span></span>일 때는 반대 방향을 나타냅니다). 이러한 특징은 차원이 증가해도 보존이 되기 때문에 다차원 공간에서 유사도 측정을 하는데 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi><mi>o</mi><mi>s</mi><mi>θ</mi></mrow><annotation encoding="application/x-tex">cos\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">cos</span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span></span></span></span>가 중요하게 사용됩니다. 이것이 유사도 측정에 가장 일반적으로 사용하는 이유가 됩니다.</li>
</ul>
<h5 class="anchor anchorWithStickyNavbar_LWe7" id="기하학적-직관">기하학적 직관<a class="hash-link" aria-label="Direct link to 기하학적 직관" title="Direct link to 기하학적 직관" href="/blog/tags/llm#기하학적-직관">​</a></h5>
<ul>
<li>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi></mrow><annotation encoding="application/x-tex">u</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">u</span></span></span></span>와 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">v</span></span></span></span>의 내적은 벡터 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi></mrow><annotation encoding="application/x-tex">u</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">u</span></span></span></span>가 벡터 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">v</span></span></span></span>로 사영되고 (반대도 성립) <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi></mrow><annotation encoding="application/x-tex">u</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">u</span></span></span></span>의 사영된 크기(<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>u</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>c</mi><mi>o</mi><mi>s</mi><mi>θ</mi></mrow><annotation encoding="application/x-tex">||u|| cos\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">∣∣</span><span class="mord mathnormal">u</span><span class="mord">∣∣</span><span class="mord mathnormal">cos</span><span class="mord mathnormal" style="margin-right:0.02778em">θ</span></span></span></span>)와 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">v</span></span></span></span>의 크기(<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>v</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">||v||</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">∣∣</span><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="mord">∣∣</span></span></span></span>)를 곱한 값으로 나타납니 다.</p>
</li>
<li>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">v</span></span></span></span>를 고정한 상태에서 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi></mrow><annotation encoding="application/x-tex">u</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">u</span></span></span></span>의 가능한 모든 회전을 시각화하면 내적은 다음을 제공합니다.</p>
<ul>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi></mrow><annotation encoding="application/x-tex">u</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">u</span></span></span></span>와 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">v</span></span></span></span>가 직교할 때 벡터 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi></mrow><annotation encoding="application/x-tex">u</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">u</span></span></span></span>가 벡터 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">v</span></span></span></span>로 사영(projection)하면 크기가 0인 벡터가 됨으로 내적은 0이 됩니다. 이것은 직관적으로 유사도가 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">0</span></span></span></span>인 것과 대응됩니다.</li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi></mrow><annotation encoding="application/x-tex">u</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">u</span></span></span></span>와 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">v</span></span></span></span>가 같은 방향이면 내적은 가장 큰 값 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi><mi>u</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>v</mi><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">|u| |v|</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">∣</span><span class="mord mathnormal">u</span><span class="mord">∣∣</span><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="mord">∣</span></span></span></span>를 갖습니다.</li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi></mrow><annotation encoding="application/x-tex">u</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">u</span></span></span></span>와 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">v</span></span></span></span>가 반대 방향이면 내적은 가장 작은 값 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mi mathvariant="normal">∣</mi><mi>u</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>v</mi><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">- |u| |v|</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">−</span><span class="mord">∣</span><span class="mord mathnormal">u</span><span class="mord">∣∣</span><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="mord">∣</span></span></span></span>를 갖습니다.</li>
</ul>
</li>
<li>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mo>⋅</mo><mi>v</mi></mrow><annotation encoding="application/x-tex">u \cdot v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4445em"></span><span class="mord mathnormal">u</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">v</span></span></span></span>를 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi></mrow><annotation encoding="application/x-tex">u</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">u</span></span></span></span>와 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">v</span></span></span></span>의 크기 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>u</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>v</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">||u|| ||v||</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">∣∣</span><span class="mord mathnormal">u</span><span class="mord">∣∣∣∣</span><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="mord">∣∣</span></span></span></span>로 나누면 범위가 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mo>−</mo><mn>1</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[-1, 1]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">[</span><span class="mord">−</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span>로 제한되어 스케일이 불변이 되고 이것이 코사인 유사도를 선정하는 이유가 되기도 합니다.</p>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="132-코사인-유사도cosine-similarity">1.3.2. 코사인 유사도(Cosine Similarity)<a class="hash-link" aria-label="Direct link to 1.3.2. 코사인 유사도(Cosine Similarity)" title="Direct link to 1.3.2. 코사인 유사도(Cosine Similarity)" href="/blog/tags/llm#132-코사인-유사도cosine-similarity">​</a></h4>
<p align="center"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mi>o</mi><mi>s</mi><mi>i</mi><mi>n</mi><mi>e</mi><mi>S</mi><mi>i</mi><mi>m</mi><mi>i</mi><mi>l</mi><mi>a</mi><mi>r</mi><mi>i</mi><mi>t</mi><mi>y</mi><mo stretchy="false">(</mo><mi>u</mi><mo separator="true">,</mo><mi>v</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>u</mi><mo>⋅</mo><mi>v</mi></mrow><mrow><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>u</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>v</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi></mrow></mfrac><mo>=</mo><mfrac><mrow><msubsup><mi mathvariant="normal">Σ</mi><mn>1</mn><mi>n</mi></msubsup><msub><mi>u</mi><mi>i</mi></msub><msub><mi>v</mi><mi>i</mi></msub></mrow><mrow><msqrt><mrow><msubsup><mi mathvariant="normal">Σ</mi><mn>1</mn><mi>n</mi></msubsup><msubsup><mi>u</mi><mi>i</mi><mn>2</mn></msubsup></mrow></msqrt><msqrt><mrow><msubsup><mi mathvariant="normal">Σ</mi><mn>1</mn><mi>n</mi></msubsup><msubsup><mi>v</mi><mi>i</mi><mn>2</mn></msubsup></mrow></msqrt></mrow></mfrac></mrow><annotation encoding="application/x-tex">CosineSimilarity(u,v) = \frac{u \cdot v}{||u|| ||v||} = \frac{ \Sigma^{n}_{1} u_i v_i }{ \sqrt{ \Sigma^n_1 u^2_i } \sqrt{ \Sigma^n_1 v^2_i } }</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.07153em">C</span><span class="mord mathnormal">os</span><span class="mord mathnormal">in</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.05764em">S</span><span class="mord mathnormal">imi</span><span class="mord mathnormal" style="margin-right:0.01968em">l</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="mord mathnormal">i</span><span class="mord mathnormal">t</span><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="mopen">(</span><span class="mord mathnormal">u</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.2251em;vertical-align:-0.52em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7051em"><span style="top:-2.655em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">∣∣</span><span class="mord mathnormal mtight">u</span><span class="mord mtight">∣∣∣∣</span><span class="mord mathnormal mtight" style="margin-right:0.03588em">v</span><span class="mord mtight">∣∣</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">u</span><span class="mbin mtight">⋅</span><span class="mord mathnormal mtight" style="margin-right:0.03588em">v</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.52em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.8568em;vertical-align:-0.8296em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0272em"><span style="top:-2.4702em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord sqrt mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0283em"><span class="svg-align" style="top:-3.4286em"><span class="pstrut" style="height:3.4286em"></span><span class="mord mtight" style="padding-left:1.19em"><span class="mord mtight"><span class="mord mtight">Σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6523em"><span style="top:-2.1885em;margin-left:0em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span><span style="top:-2.8448em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3115em"><span></span></span></span></span></span></span><span class="mord mtight"><span class="mord mathnormal mtight">u</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8051em"><span style="top:-2.1777em;margin-left:0em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-2.8448em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3223em"><span></span></span></span></span></span></span></span></span><span style="top:-3.0003em"><span class="pstrut" style="height:3.4286em"></span><span class="hide-tail mtight" style="min-width:0.853em;height:1.5429em"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.5429em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4282em"><span></span></span></span></span></span><span class="mord sqrt mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0283em"><span class="svg-align" style="top:-3.4286em"><span class="pstrut" style="height:3.4286em"></span><span class="mord mtight" style="padding-left:1.19em"><span class="mord mtight"><span class="mord mtight">Σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6523em"><span style="top:-2.1885em;margin-left:0em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span><span style="top:-2.8448em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3115em"><span></span></span></span></span></span></span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8051em"><span style="top:-2.1777em;margin-left:-0.0359em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-2.8448em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3223em"><span></span></span></span></span></span></span></span></span><span style="top:-3.0003em"><span class="pstrut" style="height:3.4286em"></span><span class="hide-tail mtight" style="min-width:0.853em;height:1.5429em"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.5429em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"></path></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4282em"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.5102em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight">Σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7385em"><span style="top:-2.214em;margin-left:0em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span><span style="top:-2.931em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286em"><span></span></span></span></span></span></span><span class="mord mtight"><span class="mord mathnormal mtight">u</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3281em"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em"><span></span></span></span></span></span></span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3281em"><span style="top:-2.357em;margin-left:-0.0359em;margin-right:0.0714em"><span class="pstrut" style="height:2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.8296em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></p>
<ul>
<li>
<p>여기서,</p>
<ul>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi></mrow><annotation encoding="application/x-tex">u</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">u</span></span></span></span>와 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">v</span></span></span></span>는 비교하고자 하는 두개의 벡터들 입니다.</li>
<li>연산 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>⋅</mo></mrow><annotation encoding="application/x-tex">\cdot</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4445em"></span><span class="mord">⋅</span></span></span></span> 는 내적(dot product)을 의미합니다.</li>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>u</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">||u||</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">∣∣</span><span class="mord mathnormal">u</span><span class="mord">∣∣</span></span></span></span>와 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>v</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">||v||</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">∣∣</span><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="mord">∣∣</span></span></span></span>는 각 벡터들의 크기 (또는 노름, norm)으로 나타내고 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">n</span></span></span></span>은 벡터의 차원을 나타냅니다.</li>
</ul>
</li>
<li>
<p>앞서 언급을 했지만 길이 정규화 부분(즉, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi><mo>⋅</mo><mi>v</mi></mrow><annotation encoding="application/x-tex">u \cdot v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4445em"></span><span class="mord mathnormal">u</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">v</span></span></span></span>를 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi></mrow><annotation encoding="application/x-tex">u</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">u</span></span></span></span>와 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">v</span></span></span></span>의 크기 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>u</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>v</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">||u|| ||v||</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">∣∣</span><span class="mord mathnormal">u</span><span class="mord">∣∣∣∣</span><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="mord">∣∣</span></span></span></span>로 나눔)은 범위를 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mo>−</mo><mn>1</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[-1,1]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">[</span><span class="mord">−</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span>로 제한하여 크기가 변하지 않게 만듭니다.</p>
</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="133-코사인-유사도-vs-내적-유사도">1.3.3. 코사인 유사도 vs. 내적 유사도<a class="hash-link" aria-label="Direct link to 1.3.3. 코사인 유사도 vs. 내적 유사도" title="Direct link to 1.3.3. 코사인 유사도 vs. 내적 유사도" href="/blog/tags/llm#133-코사인-유사도-vs-내적-유사도">​</a></h4>
<ul>
<li>
<p>코사인 유사도와 내적 유사도는 모두 텍스트 문서, 사용자 선호도 등을 나타낼 수 있는 벡터 간의 유사성을 결정하는 데 사용되는 기술입니다. 둘 사이의 선택은 특정 사용 사례와 원하는 속성에 따라 달라집니다. 다음은 내적 유사도에 비해 코사인 유사도가 얻을 수 있는 장점을 비교한 것입니다.</p>
<ul>
<li>
<p><strong>크기 정규화(Magnitude Normalization)</strong> : 코사인 유사도는 크기를 무시하고 두 벡터 사이의 각도만 고려합니다. 이는 길이가 서로 다른 문서나 크기가 유사성을 나타내지 않는 벡터를 비교할 때 특히 유용합니다. 반면에 내적 유사도는 벡터의 크기에 영향을 받습니다. 특정 용어에 대한 언급이 많은 긴 문서는 관련 콘텐츠의 비율이 낮더라도 다른 문서와 높은 내적을 가질 수 있습니다. 동일한 크기를 갖도록 데이터를 정규화하면 두 데이터를 구별할 수 없습니다. 때로는 벡터의 크기를 무시하는 것이 바람직할 때가 있고 이런 경우는 코사인 유사도가 좋은 선택이 될 수 있습니다. 하지만 벡터의 크기가 중요한 역할을 한다면 내적 유사도가 더 좋을 것입니다. 다른 말로 하 면 코사인 유사도는 크기를 정규화한 (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>∈</mo><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">\in [0,1]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span>) 벡터들의 단순한 내적으로 생각할 수 있습니다. 코사인 유사도는 크기가 불변임으로 자연스럽게 다양한 데이터 샘플에(즉, 다양한 길이) 적용될 수 있기 때문에 선호됩니다. 예를 들어 두 개의 문서 세트가 있고 각 세트 내에서 유사성을 계산한다고 가정해 보겠습니다. 그리고 각 세트 내의 문서의 내용은 유사하지만 세트 #1 문서는 세트 #2 문서보다 짧다고 가정하겠습니다. 이때 세트 #1, #2는 임베딩/특징의 크기가 다르게 나타날 수 있고 내적 유사도는 다른 값을 생성하지만 코사인 유사도는 비슷한 값을 생성합니다 (길이가 정규화 되었기 때문). 반면에, 일반 내적은 연산이 적기 때문에 (길이 정규화가 없음) &quot;저렴&quot; 합니다(복잡성과 구현 측면).</p>
</li>
<li>
<p><strong>제한된 값(Bound Values)</strong> : 코사인 유사도는 음수가 아닌 차원을 가진 벡터에 대해 -1과 1 사이의 값을 반환하고, 특히 방향이 같은 모든 벡터들에 대해 0과 1 사이의 값을 반환합  니다 (문서의 TF-IDF 표현의 경우처럼). 이런 제한된 특성은 해석을 더 쉽게 할수 있습니다. 내적 유사도는 값의 범위가 음의 무한대에서 양의 무한대이기 때문에 정규화 또는 임계값 설정을 더 어렵게 할수 있습니다.</p>
</li>
<li>
<p><strong>높은 차원에서의 견고함(Robustness in High Dimensions)</strong> : 차원이 높은 경우 대다수의 벡터들은 거의 직교에 가까운 경향을 나타내고 이는 내적을 하면 거의 0에 가까워짐을 의미한다. 하지만 코사인 유사도는 여전히 의미 있는 차이값을 제공한다. 내적은 각 차원에 따라서 그 값이 매우 민감하고 특히, 차원이 높은 경우 그 영향을 더 많이 받습니다. 벡터가 음수가 아니고 크기가 텍스트 길이에 영향을 받을 수 있는 TF-IDF와 같은 모델로 텍스트를 표현할 때 코사인 유사도가 더 적합합니다.</p>
</li>
<li>
<p><strong>일반적인 사용 사례(Common Use Cases)</strong> : 코사인 유사도는 텍스트 분석, 정보 검색 및 추천 시스템 영역에서의 효율성 때문에 해당 영역들에서 광범위하게 사용됩니다. 내적 유사도는 고유한 장점이 있지만 추가적인 정규화 없이는 이러한 사용 사례에 적합하지 않을 수 있습니다.</p>
</li>
<li>
<p><strong>직관성(Intuitiveness)</strong> : 많은 시나리오에서 각도 측면에서 생각하는 것이 원시 투영을 고려하는 것보다 더 직관적일 수 있습니다. 예를 들어, 두 벡터가 크기에 관계없이 정확히 동일한 방향을 가리키는 경우 코사인 유사도는 1이며 이는 완벽한 유사성을 나타냅니다.</p>
</li>
<li>
<p><strong>중심계산(Centroid Calculation)</strong> : 클러스터링과 같이 여러 벡터들의 중심(평균)을 계산하려고 할 때 중심은 코사인 유사도 하에서 의미 있는 상태로 유지됩니다. 벡터의 평균을 낸 다음 다른 벡터와 코사인 유사도  를 사용하여 비교하면 벡터가 &quot;평균&quot; 벡터와 얼마나 유사한지를 측정할 수 있습니다. 내적 유사도의 경우는 반드시 그런 것은 아닙니다. 이러한 장점에도 불구하고 일부 응용 프로그램(특히 신경망과 딥러닝 분야에서는) 계산 속성과 학습된 임베딩의 특성 때문에 원시 내적(때로 정규화 단계가 뒤따름)이 선호된다는 사실을 주목할 가치가 있습니다. 따라서 위의 측정방법 중 선택을 할 때는 항상 특정 응용 프로그램과 데이터 속성을 고려해야 합니다.</p>
</li>
</ul>
</li>
</ul></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a title="Overview of Large Language Models" class="tag_zVej tagRegular_sFm0" href="/blog/tags/llm">LLM</a></li></ul></div></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/blog/LLM 정리 2">LLM 정리 2</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2024-07-05T00:00:00.000Z">July 5, 2024</time> · <!-- -->11 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><div class="avatar__intro"><div class="avatar__name"><a href="https://github.com/bigdata-car" target="_blank" rel="noopener noreferrer"><span>강병수</span></a></div><small class="avatar__subtitle">데이터플랫폼연구센터 책임연구원</small></div></div></div></div></header><div class="markdown"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="2-llm은-어떻게-동작하는가">2. LLM은 어떻게 동작하는가<a class="hash-link" aria-label="Direct link to 2. LLM은 어떻게 동작하는가" title="Direct link to 2. LLM은 어떻게 동작하는가" href="/blog/tags/llm#2-llm은-어떻게-동작하는가">​</a></h2>
<ul>
<li>
<p>개요에서 언급한 것 처럼 LLM은 이전 토큰 세트를 기반으로 다음 토  큰을 예측하도록 훈련되었습니다. 이는 생성기능을 활성화 하는 자동회귀 방식(autoregressive, 현재 생성된 토큰은 다음 토큰을 생성하기 위한 입력으로 거대언어모델에 재입력 됨)을 수행하여 생성을 가능하게 합니다.</p>
</li>
<li>
<p>첫 번째 단계에서 받은 프롬프트를 토큰화 하고 이를 임베딩으로 변환하는 작업이 수행 됩니다. 임베딩은 입력 텍스트의 벡터 표현입니다. 이러한 임베딩은 무작위로 초기화되어 입력 토큰의 비 의미론적인 벡터 형태를 나타냅니다. 그리고, 모델 훈련 과정에서 맥락화되는 학습이 수행됩니다.</p>
</li>
<li>
<p>다음으로, 레이어별 어텐션(attentation) 및 피드포워드 연산을 수행하여 최종적으로 어휘의 각 단어에 숫자 또는 로짓(logit)을 출력하거나(GPT-N, LLaMA 등의 디코더 모델) 의미론적 임베딩을 출력합니다(BERT와 같은 인코더 모델 및 RoBERTa, ELECTRA 등과 같은 변형 모델).</p>
</li>
<li>
<p>마지막으로 디코더 모델의 경우 다음 단계는 각 (정규화되지 않은) 로짓을 (정규화된) 확률 분포(Softmax 함수를 통해)로 변환하여 텍스트에서 다음에 올 단어를 결정하는 것입니다.</p>
</li>
<li>
<p>아래와 같이 단계를 더 자세히 살펴보겠습니다 :</p>
<ol>
<li>
<p><strong>토큰화</strong> :</p>
<ul>
<li>LLM이 처리를 하기 전에 원시 입력 텍스트는 더 작은 단위(종종 하위 단어 또는 단어)로 토큰화 하여 모델이 인식할 수 있는 조각으로 입력을 나눕니다.</li>
<li>모델에는 고정된 어휘목록(vocabulary)이 있습니다. 따라서, 토큰화 단계는 입력이 어휘목록과 일치하는 형식이 되도록 보장하기 때문에 매우 중요합니다.</li>
<li>GPT-3.5 및 GPT-4용 OpenAI 토크나이저는 <a href="https://platform.openai.com/tokenizer" target="_blank" rel="noopener noreferrer">여기</a>에서 찾을 수 있습니다.</li>
<li>자세한 내용은 <a href="https://aman.ai/primers/ai/tokenizer/" target="_blank" rel="noopener noreferrer">토큰화에 대한 입문서</a>를 참조하세요.</li>
</ul>
</li>
<li>
<p><strong>임베딩</strong> :</p>
<ul>
<li>각 토큰은 임베딩 매트릭스를 사용하여 고차원 벡터에 매핑됩니다. 이 벡터 표현은 토큰의 맥락적 의미를 포착하며 모델의 다음 레이어에 입력으로 사용됩니다.</li>
<li>토큰의 순서에 대한 정보를 모델에 제공하기 위해 매핑된 임베딩에 위치 인코딩(positional encoding)이 추가됩니다. 이는 트랜스포머와 같은 모델이 고유한 순서 인식을 갖고 있지 않기 때문에 특히 중요합니다.</li>
</ul>
</li>
<li>
<p><strong>트랜스포머 구조</strong> :</p>
<ul>
<li>대부분의 최신 LLM의 핵심은 트랜스포머 구조입니다.</li>
<li>트랜스포머는 여러 레이어로 구성되어 있으며, 각 레이어에는 두 가지 주요 구성 요소가 있습니다 : multi-head self-attention 메커니즘과 position-wise feed-forward network 입니다.</li>
<li>자기 어텐션 메커니즘(self-attention mechanism)은 각 토큰들이 자신과 관련해 중요성을 갖는 다른 토큰들에게 가중치를 부여할 수 있게 합니다. 이는 본질적으로 주어진 토큰과 관련있는 특정 부분에 대해 모델이 &quot;주의를 기울일&quot; 수 있도록 합니다.</li>
<li>어텐션 연산된 결과는, 각 위치에서 독립적으로 피드포워드 신경망으로 전달됩니다.</li>
<li>자세한 내용은 <a href="https://aman.ai/primers/ai/transformers/" target="_blank" rel="noopener noreferrer">트랜스포머 아키텍처에 대한 입문서</a>를 참조하세요.</li>
</ul>
</li>
<li>
<p><strong>잔차연결 (Residual Connection)</strong> :</p>
<ul>
<li>모델의 각 하위 계층(예: 자기 어텐션 또는 피드포워드 신경망)은 주변에 잔여 연결이 적용된 후 계층 정규화가 수행됩니다. 이는 활성화를 안정화하고 훈련 속도를 높이는 데 도움이 됩니다.</li>
</ul>
</li>
<li>
<p><strong>출력 레이어</strong> :</p>
<ul>
<li>모든 트랜스포머 레이어를 통과한 후, 각 토큰의 최종 표현은 모델의 어휘목록에 있는 각 단어에 대응하는 로짓 벡터로 변환됩니다.</li>
<li>이러한 로짓은 어휘 목록의 각 단어들이 시퀀스의 다음 단어가 될 가능성을 설명합니다.</li>
</ul>
</li>
<li>
<p><strong>확률분포</strong> :</p>
<ul>
<li>로짓을 확률로 변환하기 위해 Softmax 함수가 적용됩니다. 이는 모두 0과 1 사이에 있고 합이 1이 되도록 로짓을 정규화합니다.</li>
<li>어휘 목록의 단어들 중 확률이 가장 높은 단어가 시퀀스의 다음 단어로 선택될 수 있습니다.</li>
</ul>
</li>
<li>
<p><strong>디코딩 (Decoding)</strong> :</p>
<ul>
<li>적용되는 상황에 따라 일관되고 문맥에 맞는 시퀀스를 생성하기 위하여, 그리디 디코딩(greedy decoding), 빔 검색(beam search), Top-K 샘플링(top-k sampling)과 같은 다양한 디코딩 전략이 사용됩니다.</li>
<li>자세한 내용은 <a href="https://aman.ai/primers/ai/token-sampling/" target="_blank" rel="noopener noreferrer">토큰 샘플링 방법</a>에 대한 입문서를 참조하세요.</li>
</ul>
</li>
</ol>
</li>
<li>
<p>여러 단계의 프로세스를 통해, LLM은 인간과 유사한 텍스트를 생성하고, 맥락을 이해하고, 프롬프트에 대한 관련 응답이나 완성을 제공할 수 있습니다.</p>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="21-llm-학습-단계">2.1. LLM 학습 단계<a class="hash-link" aria-label="Direct link to 2.1. LLM 학습 단계" title="Direct link to 2.1. LLM 학습 단계" href="/blog/tags/llm#21-llm-학습-단계">​</a></h3>
<ul>
<li>상위 수준에서, LLMs의 훈련에 포함되는 단계는 다음과 같습니다:<!-- -->
<ol>
<li><strong>문서(코퍼스, corpus) 준비</strong> : 뉴스 기사, 소셜 미  디어 게시물, 웹 문서 등 대규모 텍스트 데이터 모음을 수집합니다.</li>
<li><strong>토큰화</strong> : 텍스트를 토큰이라고 하는 개별 단어 또는 하위 단어로 분할합니다.</li>
<li><strong>임베딩 생성</strong> : 일반적으로 훈련을 처음 시작할 때 PyTorch의 <a href="https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html" target="_blank" rel="noopener noreferrer"><code>nn.Embedding</code></a> 클래스를 통해 랜덤하게 초기화된 임베딩 테이블을 사용합니다. 또한, Word2Vec, GloVe, FastText 등과 같은 사전 훈련된 임베딩도 사용할 수 있습니다. 이러한 임베딩은 입력 토큰의 맥락화되지 않은 벡터 형식을 나타냅니다.</li>
<li><strong>신경망 훈련</strong> : 입력 토큰에 대한 신경망 모델을 훈련합니다.<!-- -->
<ul>
<li>BERT 및 그 변형과 같은 인코더 모델의 경우 모델은 마스킹된 특정 단어의 전후 맥락(주변 단어)을 예측하는 방법을 학습합니다.</li>
<li>BERT는 특히 마스킹된 단어를 예측하는 마스크드 언어 모델링 작업(Masked Language Modeling task 또는 Cloze task)과 다음 문장 예측 작업으로 훈련되었습니다; <a href="https://aman.ai/primers/ai/bert/" target="_blank" rel="noopener noreferrer">BERT 입문서</a>에 설명되어 있습니다.</li>
<li>GPT-N, LLaMA 등과 같은 디코더 모델의 경우 주어진 이전 토큰들의 맥락을 고려하여 시퀀스의 다음 토큰을 예측하는 방법을 학습합니다.</li>
</ul>
</li>
</ol>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="22-추론-reasoning">2.2. 추론 (Reasoning)<a class="hash-link" aria-label="Direct link to 2.2. 추론 (Reasoning)" title="Direct link to 2.2. 추론 (Reasoning)" href="/blog/tags/llm#22-추론-reasoning">​</a></h3>
<ul>
<li>LLM에서 추론이 어떻게 작동하는지 살펴보겠습니다; 우리는 추론을 “증거와 논리를 사용하여 추론하는 능력”으로 정의할 것입니다. <a href="https://arxiv.org/pdf/2302.07842" target="_blank" rel="noopener noreferrer">(source)</a></li>
<li>추론에는 상식적 추론이나 수학적 추론과 같이 다양한 종류가 있습니다.</li>
<li>마찬가지로, 모델에서 추론을 이끌어내는 방법 또한 다양하며 그 중 하나는 여기에서 언급하는 생각의 사슬(chain-of-thought) 프롬프팅 입니다.</li>
<li>추론과 사실적 정보를 분리하여 최종 결과에 대한 기여도를 분석하는 것은 간단한 일이 아니기 때문에, LLM이 최종 예측을 위해 얼마나 많은 추론을 하는지 아직 알 수 없다는 점을 유의하는 것이 중요합니다.</li>
</ul></div><footer class="row docusaurus-mt-lg"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a title="Overview of Large Language Models" class="tag_zVej tagRegular_sFm0" href="/blog/tags/llm">LLM</a></li></ul></div></footer></article><nav class="pagination-nav" aria-label="Blog list page navigation"></nav></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/manual/intro">KADaP 사용 메뉴얼</a></li></ul></div><div class="col footer__col"><div class="footer__title">서비스</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://portal.bigdata-car.kr" target="_blank" rel="noopener noreferrer" class="footer__link-item">자동차 데이터 포털</a></li><li class="footer__item"><a href="https://cloud.bigdata-car.kr" target="_blank" rel="noopener noreferrer" class="footer__link-item">자동차 산업 클라우드</a></li><li class="footer__item"><a href="https://ide.bigdata-car.kr" target="_blank" rel="noopener noreferrer" class="footer__link-item">GPUaaS</a></li><li class="footer__item"><a href="https://api.bigdata-car.kr" target="_blank" rel="noopener noreferrer" class="footer__link-item">마켓 플레이스</a></li><li class="footer__item"><a href="https://agent.bigdata-car.kr" target="_blank" rel="noopener noreferrer" class="footer__link-item">자동차 산업 에이전트</a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2026 KATECH(Korea Automotive Technology Institute) All right reserved</div></div></div></footer></div>
</body>
</html>